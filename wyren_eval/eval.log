这里写一些我的总体评估实验计划，具体每个方法的评估实验的记录在各自方法的 eval.log 中。
- [] 0MEMOIR
- [] 1SAKE
- [] 6SIMIE 
- [] 7RLEdit 
- [] 9FastMEMIT 
- [] 10UNKE


//跑一下之前benchmark的wise实验：
    python /root/autodl-tmp/benchmark/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py \
        --editing_method WISE \
        --hparams_dir /root/autodl-tmp/benchmark/Knowledge-Editing-Benchmark/EasyEdit/hparams/WISE/llama3.1-8b.yaml \
        --data_dir /root/autodl-tmp/benchmark/Knowledge-Editing-Benchmark/wyren/dataset/zsre/ZsRE-test-all.json \
        --output_dir /root/autodl-tmp/benchmark/Knowledge-Editing-Benchmark/wyren/1outputs/benchmark \
        --train_data_path /root/autodl-tmp/benchmark/Knowledge-Editing-Benchmark/wyren/dataset/zsre/zsre_train_10000.json \
        --datatype zsre \
        --ds_size 10 \
        --start_index 0 \
        --end_index 10 \
        --sequential_edit \
        >/root/autodl-tmp/wyren_eval/log/run_WISE_zsre_10_rewrite_portability.log 2>&1

python /root/autodl-tmp/benchmark/Knowledge-Editing-Benchmark/EasyEdit/examples/run_LLM_evaluation.py \
    --editing_method WISE \
    --hparams_dir /root/autodl-tmp/benchmark/Knowledge-Editing-Benchmark/EasyEdit/hparams/WISE/llama3.1-8b.yaml \
    --data_dir /root/autodl-tmp/benchmark/Knowledge-Editing-Benchmark/wyren/dataset/zsre/ZsRE-test-all.json \
    --output_dir /root/autodl-tmp/benchmark/Knowledge-Editing-Benchmark/wyren/1outputs/benchmark \
    --datatype zsre \
    --ds_size 10 \
    --start_index 0 \
    --end_index 10 \
    --sequential_edit \
    >/root/autodl-tmp/wyren_eval/log/run_WISE_zsre_10_rewrite_portability.log 2>&1

预估会出现评估模型vllm没有启动相关的报错，先看一下输出先。
成功跑通评估模型之前的步骤，目前的输出存档在/root/autodl-tmp/wyren_eval/log/run_WISE_zsre_10_rewrite_portability_novllm.log

随后的计划：
1. 配置评估模型服务器，使用 qwen2.5-7b-instruct 模型，跑通wise的评估pipeline
2. 在跑通wise评估的基础上，跑通memior的评估

启动服务器：
conda activate vllm_env
要开启代理连higgingface
bash clash-for-linux-master/start.sh
bash /root/autodl-tmp/benchmark/Knowledge-Editing-Benchmark/vllm_serve.sh

rm -rf /root/.cache/torch
rm -rf /root/.cache/huggingface
rm -rf /root/.cache/vllm

现在的情况是显卡不够用，至少得租两张A800才能跑起来
明天的任务是把现在的模型环境打包、上传github，重新租两张卡跑实验